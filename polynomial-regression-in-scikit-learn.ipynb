{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lxlz1986/polynomial-regression-in-scikit-learn?scriptVersionId=143031423\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## 1. Why we need Polynomial Regression?\n\n[Linear Regresspion](https://www.kaggle.com/code/lxlz1986/linear-regression-model-in-scikit-learn) is a simple model to make predictions if the relationship between target and features in a dataset is linear or close to linear. \n\nHowever, most of the time, our data is much more complex that it is not sufficient to represent the relationship just using a straight line. In the cases with **non-linear** data, we can try to use polynomial to simulate the relationship between the output(prediction) and input(features), such as:\n\n$$\\hat{y} = \\theta_0 + \\theta_1x +\\theta_2x^2$$\n\nwhere $\\hat{y}$ is the prediction, $x$ is the original feature and $x^2$ is the extended feature, which is the square of the original feature.\n\nFrom the mathematical aspect, we can take this polynomial equation as a liner model in which $x$ and $x^2$ are two features of a data point. This makes it possible that we can train a linear model to fit the non-linear data. This method to make predictions is called **polynomial regression**.","metadata":{}},{"cell_type":"markdown","source":"## 2. How to train Polynomial Regression model using Linear Regression model?\n\nAs mentioned above, we can train a liner model to fit non-liner data by extend the single feature to multiple features. \nThere are many ways to extend the feature, and one straight way is to add powers to the original feature. \n\nNow, we will look at how it works in Scikit-Learn.","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Import necessary packages","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\n\nimport matplotlib as mpl # ploting\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression  # liner regression model\nfrom sklearn.preprocessing import PolynomialFeatures # polynommial features(extended features)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T19:09:16.732719Z","iopub.execute_input":"2023-09-09T19:09:16.733247Z","iopub.status.idle":"2023-09-09T19:09:17.719754Z","shell.execute_reply.started":"2023-09-09T19:09:16.733207Z","shell.execute_reply":"2023-09-09T19:09:17.718672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Generate non-linear dataset based on 2-degree polynomial ","metadata":{}},{"cell_type":"code","source":"n = 100 # 100 data points\nX = 6 * np.random.rand(n,1)-4\ny = X**2 + 2 * X + 3 + np.random.rand(n,1)\n\n# plotting the dataset\nplt.plot(X,y,'b.')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Non-linear Dataset')\nplt.axis([-3,3,0.5,12])\nplt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T19:09:20.499372Z","iopub.execute_input":"2023-09-09T19:09:20.500899Z","iopub.status.idle":"2023-09-09T19:09:20.91846Z","shell.execute_reply.started":"2023-09-09T19:09:20.500829Z","shell.execute_reply":"2023-09-09T19:09:20.916969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see that the generated dataset is obviously non-linear. ","metadata":{}},{"cell_type":"markdown","source":"### 2.3 Transforme dataset to have polynomial features\nIn oder to have polynomial features, we add square of each feature as new features. This can be down with the help of `PolynomialFeature` and `fit_transform` in Scikit-Learn.","metadata":{}},{"cell_type":"code","source":"poly_features = PolynomialFeatures(degree=2) # decide the maximal degree of the polynomial feature\nX_ploy = poly_features.fit_transform(X) # convert the original feature to polynomial feature\n\n\n# check the extened polynomial features of the first data point\nprint('original feature:', X[0])\nprint('polynomial features',X_ploy[0])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-09T19:31:16.560705Z","iopub.execute_input":"2023-09-09T19:31:16.561821Z","iopub.status.idle":"2023-09-09T19:31:16.569805Z","shell.execute_reply.started":"2023-09-09T19:31:16.56178Z","shell.execute_reply":"2023-09-09T19:31:16.568516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4 Use new polynomial features to train the Linear Regression model\n\nNow we can use the above obtained polynomial features to train the Linear Regression model.","metadata":{}},{"cell_type":"code","source":"lin_reg = LinearRegression()\nlin_reg.fit(X_ploy,y)\nlin_reg.intercept_, lin_reg.coef_ # check the bais term and feature weights of the trained model","metadata":{"execution":{"iopub.status.busy":"2023-09-09T19:20:12.06051Z","iopub.execute_input":"2023-09-09T19:20:12.061053Z","iopub.status.idle":"2023-09-09T19:20:12.096421Z","shell.execute_reply.started":"2023-09-09T19:20:12.061012Z","shell.execute_reply":"2023-09-09T19:20:12.095475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5 Use the trained Linear Regression model to make predictions","metadata":{}},{"cell_type":"code","source":"X_new = np.sort(X,axis = 0) # in order to plot the line of the model, we need to sort the the value of x-axis\nX_new_ploy = poly_features.fit_transform(X_new) # compute the polynomial features \ny_predict = lin_reg.predict(X_new_ploy) # make predictions using trained Linear Regression model\n\n# plot the original dataset and the prediction results\nfig,ax = plt.subplots()\nax.plot(X,y,'b.', label = 'Training date samples')\nax.plot(X_new,y_predict,'g-',linewidth=2, label = 'Predictions')\nax.axis([-3,4,0.5,12])\nax.set_xlabel('X')\nax.set_ylabel('y')\nax.legend()\nax.grid(True)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T19:40:01.083793Z","iopub.execute_input":"2023-09-09T19:40:01.084342Z","iopub.status.idle":"2023-09-09T19:40:01.482632Z","shell.execute_reply.started":"2023-09-09T19:40:01.084302Z","shell.execute_reply":"2023-09-09T19:40:01.481172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the comparasion we see that, the trained polynomial model fits the training dataset actually very well.","metadata":{}}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}